{
 "cells": [
  {
   "cell_type": "raw",
   "id": "73c34bb7",
   "metadata": {},
   "source": [
    "0: left, 1: right, 2: forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d901a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name = 'pretrain.spirl_v5_skill3_z10_1kiq585w_simple_c_v2_B0200_20_C_5000.pt'\n",
    "skill_length = 3\n",
    "frame_stack = 5\n",
    "resize_res = 28\n",
    "is_quantized = True\n",
    "num_level = 0\n",
    "# thresholds = list()\n",
    "thresholds = [10 * i for i in range(0, 20)]\n",
    "pos = False"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5b97338",
   "metadata": {},
   "source": [
    "a: frame stack 5 -> 10\n",
    "b: quantization\n",
    "c: low resolution 84 -> 28"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1415ba87",
   "metadata": {},
   "source": [
    "After adding logsoftmax (1k step)\n",
    "A:\n",
    "B10-100-200: 43 why?\n",
    "B10: 11\n",
    "C: 7\n",
    "\n",
    "prior ViT:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "319a1c87",
   "metadata": {},
   "source": [
    "https://github.com/msgpack-rpc/msgpack-rpc-python/issues/24\n",
    "https://github.com/xaedes/msgpack-rpc-python/tree/with_tornado_453/msgpackrpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d56ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee Geonju\\miniconda3\\envs\\py39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as torch_dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "# modified output ViT; from (batch, num_class) to (batch, dim)\n",
    "\n",
    "# from simpl.collector import ConcurrentCollector, TimeLimitCollector, GPUWorker, Buffer\n",
    "# from simpl.nn import itemize\n",
    "# from simpl.math import discount\n",
    "# from simpl.rl.policy import ContextTruncatedNormalMLPPolicy\n",
    "# from simpl.rl.qf import MLPQF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47762ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Lee Geonju\\\\Desktop\\\\drone\\\\SiMPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ebbe75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LowPolicy(\n",
       "  (lstm): LSTM(20, 128, proj_size=3, num_layers=2, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from simpl.nn import MLP\n",
    "\n",
    "from simpl.nn import ToDeviceMixin\n",
    "import torch.distributions as torch_dist\n",
    "\n",
    "from simpl.math import inverse_softplus, inverse_sigmoid\n",
    "\n",
    "class SkillEncoder(ToDeviceMixin, nn.Module):\n",
    "    def __init__(self, action_dim, z_dim, hidden_dim, n_lstm, n_mlp_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            action_dim,\n",
    "            hidden_dim, n_lstm, batch_first=True\n",
    "        )\n",
    "        self.mlp = MLP([hidden_dim]*n_mlp_hidden + [2*z_dim], 'relu')\n",
    "        \n",
    "        self.register_buffer('prior_loc', torch.zeros(z_dim))\n",
    "        self.register_buffer('prior_scale', torch.ones(z_dim))\n",
    "        # self.register_buffer('h0', torch.zeros(n_lstm, hidden_dim))\n",
    "        # self.register_buffer('c0', torch.zeros(n_lstm, hidden_dim))\n",
    "\n",
    "    @property\n",
    "    def prior_dist(self):\n",
    "        return torch_dist.Independent(torch_dist.Normal(self.prior_loc, self.prior_scale), 1)\n",
    "    \n",
    "        \n",
    "    def dist(self, batch_seq_action):\n",
    "        # batch_h0 = self.h0[:, None, :].expand(-1, len(batch_seq_state), -1)\n",
    "        # batch_c0 = self.c0[:, None, :].expand(-1, len(batch_seq_state), -1)\n",
    "        \n",
    "        batch_seq_onehot_action = F.one_hot(batch_seq_action, num_classes=self.action_dim).float()\n",
    "        batch_seq_out, _ = self.lstm(batch_seq_onehot_action)\n",
    "        batch_last_out = batch_seq_out[:, -1, :]\n",
    "        batch_loc, batch_pre_scale = self.mlp(batch_last_out).chunk(2, dim=-1)\n",
    "        batch_scale = F.softplus(batch_pre_scale)\n",
    "        \n",
    "        return torch_dist.Independent(\n",
    "            torch_dist.Normal(batch_loc, batch_scale)\n",
    "        , 1)\n",
    "\n",
    "    \n",
    "class PriorPolicy(ToDeviceMixin, nn.Module):\n",
    "    def __init__(self, state_shape, z_dim, hidden_dim, n_hidden,\n",
    "                 min_scale=0.001, max_scale=None, init_scale=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "#         assert state_shape == (5, 84, 84)\n",
    "#         assert hidden_dim == 128\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        if resize_res == 84:\n",
    "            self.conv_net = nn.Sequential(\n",
    "                nn.Conv2d(frame_stack, 32, kernel_size=4, stride=3),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=4, stride=3),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "            )\n",
    "            \n",
    "        elif resize_res == 28:\n",
    "            self.conv_net = nn.Sequential(\n",
    "                nn.Conv2d(frame_stack, 64, kernel_size=4, stride=3),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "            )\n",
    "            \n",
    "        self.mlp = MLP([128]*n_hidden + [2*z_dim], 'relu')\n",
    "        \n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        \n",
    "        if max_scale is None:\n",
    "            self.pre_init_scale = inverse_softplus(init_scale)\n",
    "        else:\n",
    "            self.pre_init_scale = inverse_sigmoid(init_scale / max_scale)\n",
    "        \n",
    "        \n",
    "#     def dist(self, batch_state, batch_pos):\n",
    "#         input_dim = batch_state.dim()\n",
    "#         if input_dim > 4:\n",
    "#             batch_shape = batch_state.shape[:-3] \n",
    "#             data_shape = batch_state.shape[-3:]\n",
    "#             batch_state = batch_state.view(-1, *data_shape)\n",
    "#         batch_h = self.conv_net(batch_state)[..., 0, 0]\n",
    "#         if pos:\n",
    "#             batch_h = torch.cat((batch_h, batch_pos), dim=-1)\n",
    "\n",
    "#         batch_loc, batch_pre_scale = self.mlp(batch_h).chunk(2, dim=-1)\n",
    "\n",
    "#         if self.max_scale is None:\n",
    "#             batch_scale = self.min_scale + F.softplus(self.pre_init_scale + batch_pre_scale)\n",
    "#         else:\n",
    "#             batch_scale = self.min_scale + self.max_scale*torch.sigmoid(self.pre_init_scale + batch_pre_scale)\n",
    "        \n",
    "#         if input_dim > 4:\n",
    "#             batch_loc = batch_loc.view(*batch_shape, self.z_dim)\n",
    "#             batch_scale = batch_scale.view(*batch_shape, self.z_dim)\n",
    "        \n",
    "#         return torch_dist.Independent(\n",
    "#             torch_dist.Normal(batch_loc, batch_scale)\n",
    "#         , 1)\n",
    "    \n",
    "    \n",
    "    def dist(self, batch_state):\n",
    "        input_dim = batch_state.dim()\n",
    "        if input_dim > 4:\n",
    "            batch_shape = batch_state.shape[:-3] \n",
    "            data_shape = batch_state.shape[-3:]\n",
    "            batch_state = batch_state.view(-1, *data_shape)\n",
    "        batch_h = self.conv_net(batch_state)[..., 0, 0]\n",
    "        batch_loc, batch_pre_scale = self.mlp(batch_h).chunk(2, dim=-1)\n",
    "\n",
    "        if self.max_scale is None:\n",
    "            batch_scale = self.min_scale + F.softplus(self.pre_init_scale + batch_pre_scale)\n",
    "        else:\n",
    "            batch_scale = self.min_scale + self.max_scale*torch.sigmoid(self.pre_init_scale + batch_pre_scale)\n",
    "        \n",
    "        if input_dim > 4:\n",
    "            batch_loc = batch_loc.view(*batch_shape, self.z_dim)\n",
    "            batch_scale = batch_scale.view(*batch_shape, self.z_dim)\n",
    "        \n",
    "        return torch_dist.Independent(\n",
    "            torch_dist.Normal(batch_loc, batch_scale)\n",
    "        , 1)\n",
    "\n",
    "\n",
    "# class PriorPolicy(ToDeviceMixin, nn.Module):\n",
    "#     def __init__(self, state_shape, z_dim, hidden_dim, n_hidden,\n",
    "#                  min_scale=0.001, max_scale=None, init_scale=0.1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         assert state_shape == (5, 84, 84)\n",
    "#         assert hidden_dim == 128\n",
    "        \n",
    "#         self.z_dim = z_dim\n",
    "#         self.vit = ViT(\n",
    "#             image_size = 84,\n",
    "#             patch_size = 21,\n",
    "#             num_classes = 3,\n",
    "#             dim = hidden_dim,\n",
    "#             depth = 3,\n",
    "#             heads = 8,\n",
    "#             mlp_dim = 256,\n",
    "#             dropout = 0.1,\n",
    "#             emb_dropout = 0.1,\n",
    "#             channels = 5,\n",
    "#         )\n",
    "#         self.mlp = MLP([128]*n_hidden + [2*z_dim], 'relu')\n",
    "        \n",
    "#         self.min_scale = min_scale\n",
    "#         self.max_scale = max_scale\n",
    "        \n",
    "#         if max_scale is None:\n",
    "#             self.pre_init_scale = inverse_softplus(init_scale)\n",
    "#         else:\n",
    "#             self.pre_init_scale = inverse_sigmoid(init_scale / max_scale)\n",
    "        \n",
    "#     def dist(self, batch_state):\n",
    "#         input_dim = batch_state.dim()\n",
    "#         batch_shape = batch_state.shape[:-3] \n",
    "#         data_shape = batch_state.shape[-3:]\n",
    "#         batch_state = batch_state.view(-1, *data_shape)\n",
    "#         batch_h = self.vit(batch_state)\n",
    "#         batch_loc, batch_pre_scale = self.mlp(batch_h).chunk(2, dim=-1)\n",
    "\n",
    "#         if self.max_scale is None:\n",
    "#             batch_scale = self.min_scale + F.softplus(self.pre_init_scale + batch_pre_scale)\n",
    "#         else:\n",
    "#             batch_scale = self.min_scale + self.max_scale*torch.sigmoid(self.pre_init_scale + batch_pre_scale)\n",
    "        \n",
    "#         if input_dim > 4:\n",
    "#             batch_loc = batch_loc.view(*batch_shape, self.z_dim)\n",
    "#             batch_scale = batch_scale.view(*batch_shape, self.z_dim)\n",
    "        \n",
    "#         return torch_dist.Independent(\n",
    "#             torch_dist.Normal(batch_loc, batch_scale)\n",
    "#         , 1)\n",
    "    \n",
    "# class GELU(torch.nn.Module):\n",
    "#     def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "#         return torch.nn.functional.gelu(input)\n",
    "# torch.nn.modules.activation.GELU = GELU\n",
    "        \n",
    "    \n",
    "# class LowPolicy(ToDeviceMixin, nn.Module):\n",
    "#     def __init__(self, state_shape, action_dim, z_dim, hidden_dim, n_hidden):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         assert state_shape == (5, 84, 84)\n",
    "        \n",
    "#         self.action_dim = action_dim\n",
    "#         self.conv_net = nn.Sequential(\n",
    "#             nn.Conv2d(5, hidden_dim, kernel_size=4, stride=3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(hidden_dim, hidden_dim, kernel_size=4, stride=3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(hidden_dim, hidden_dim, kernel_size=4, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1),\n",
    "#         )\n",
    "#         self.mlp = MLP([hidden_dim + z_dim] + [hidden_dim]*(n_hidden-1) + [action_dim], 'relu')\n",
    "        \n",
    "#     def dist(self, batch_state, batch_z):\n",
    "#         input_dim = batch_state.dim()\n",
    "#         if input_dim > 4:\n",
    "#             batch_shape = batch_state.shape[:-3]\n",
    "            \n",
    "#             data_shape = batch_state.shape[len(batch_shape):]\n",
    "#             batch_state = batch_state.view(-1, *data_shape)\n",
    "\n",
    "#             data_shape = batch_z.shape[len(batch_shape):]\n",
    "#             batch_z = batch_z.reshape(-1, *data_shape)\n",
    "        \n",
    "#         batch_h = self.conv_net(batch_state)[..., 0, 0]\n",
    "#         batch_h_z = torch.cat([batch_h, batch_z], dim=-1)\n",
    "#         batch_logits = self.mlp(batch_h_z)\n",
    "        \n",
    "#         if input_dim > 4:\n",
    "#             batch_logits = batch_logits.view(*batch_shape, self.action_dim)\n",
    "        \n",
    "#         return torch_dist.Categorical(logits=batch_logits)\n",
    "\n",
    "\n",
    "class LowPolicy(ToDeviceMixin, nn.Module):\n",
    "    def __init__(self, action_dim, z_dim, hidden_dim, n_lstm):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hidden_dim == 128\n",
    "         \n",
    "        self.action_dim = action_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            z_dim,\n",
    "            hidden_dim, n_lstm, batch_first=True, proj_size=action_dim,\n",
    "        )\n",
    "        \n",
    "    def dist(self, batch_z):\n",
    "        batch_logits, _  = self.lstm(batch_z)     \n",
    "        return torch_dist.Categorical(logits=batch_logits)\n",
    "\n",
    "\n",
    "# class LowPolicy(ToDeviceMixin, nn.Module):\n",
    "#     def __init__(self, action_dim, z_dim, hidden_dim, n_lstm):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         assert hidden_dim == 128\n",
    "        \n",
    "#         self.action_dim = action_dim\n",
    "#         self.vit = ViT(\n",
    "#             image_size = 84,\n",
    "#             patch_size = 21,\n",
    "#             num_classes = 3,\n",
    "#             dim = 256,\n",
    "#             depth = 3,\n",
    "#             heads = 8,\n",
    "#             mlp_dim = 256,\n",
    "#             dropout = 0.1,\n",
    "#             emb_dropout = 0.1,\n",
    "#             channels = 5,\n",
    "#         )\n",
    "\n",
    "#         self.mlp = MLP([256 + z_dim] + [256]*(2-1) + [action_dim], 'relu')\n",
    "        \n",
    "#     def dist(self, batch_state, batch_z):\n",
    "        \n",
    "# #         input_dim = batch_state.dim()\n",
    "# #         if input_dim > 4:\n",
    "            \n",
    "#         # batch_state = torch.flatten(batch_state, start_dim=0, end_dim=1)\n",
    "#         # batch_z = torch.flatten(batch_z, start_dim=0, end_dim=1)\n",
    "#         data_shape = batch_state.shape[:-3]\n",
    "#         batch_state = batch_state.reshape(-1, *batch_state.shape[-3:])\n",
    "#         batch_z = batch_z.reshape(-1, *batch_z.shape[-1:])\n",
    "#         batch_logits = self.vit(batch_state)\n",
    "#         print(batch_logits.shape, batch_z.shape)\n",
    "#         batch_logits = torch.cat([batch_logits, batch_z], dim=1)\n",
    "#         batch_logits = self.mlp(batch_logits)\n",
    "#         batch_logits = F.log_softmax(batch_logits, dim=-1)\n",
    "#         batch_logits = batch_logits.reshape(*data_shape, -1)\n",
    "#         return torch_dist.Categorical(logits=batch_logits)\n",
    "    \n",
    "# class GELU(torch.nn.Module):\n",
    "#     def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "#         return torch.nn.functional.gelu(input)\n",
    "# torch.nn.modules.activation.GELU = GELU\n",
    "\n",
    "\n",
    "load = torch.load('../checkpoints/'+ckpt_name, map_location='cpu')\n",
    "prior_policy = load['prior_policy']\n",
    "low_policy = load['low_policy']\n",
    "prior_policy.eval()\n",
    "low_policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7393c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('C:\\\\Users\\\\Lee Geonju\\\\Desktop\\\\drone\\\\PythonClient')\n",
    "sys.path.append('C:\\\\Users\\\\Lee Geonju\\\\Desktop\\\\drone\\\\PythonClient\\\\reinforcement_learning')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import multirotor.setup_path\n",
    "import airsim\n",
    "import numpy as np\n",
    "import math\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import wandb\n",
    "import airsim\n",
    "import numpy as np\n",
    "import torch\n",
    "from airgym.envs.airsim_env import AirSimEnv\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class AirSimDroneEnv(AirSimEnv):\n",
    "    def __init__(self, ip_address, step_length, image_shape):\n",
    "        super().__init__(image_shape)\n",
    "        self.step_length = step_length\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "        self.state = {\n",
    "            \"position\": np.zeros(3),\n",
    "            \"collision\": False,\n",
    "            \"prev_position\": np.zeros(3),\n",
    "        }\n",
    "\n",
    "        self.drone = airsim.MultirotorClient(ip=ip_address)\n",
    "        self.action_space = spaces.Discrete(7)\n",
    "        self._setup_flight()\n",
    "\n",
    "        self.image_request = airsim.ImageRequest(\n",
    "            \"0\", airsim.ImageType.DepthVis, True, False\n",
    "            #\"0\", airsim.ImageType.DepthPlanar, True, False\n",
    "        )\n",
    "\n",
    "    def __del__(self):\n",
    "        self.drone.reset()\n",
    "\n",
    "    def _setup_flight(self):\n",
    "        self.drone.reset()\n",
    "        self.drone.enableApiControl(True)\n",
    "        self.drone.armDisarm(True)\n",
    "        \n",
    "        # Set home position and velocity\n",
    "        self.drone.takeoffAsync().join()\n",
    "        #self.drone.moveToPositionAsync(212, -320, -19.0225, 10).join()\n",
    "        #self.drone.moveByVelocityAsync(1, -0.67, -0.8, 5).join()\n",
    "\n",
    "    def transform_obs(self, responses):\n",
    "        img1d = np.array(responses[0].image_data_float, dtype=np.float)\n",
    "        img1d = 255*img1d.clip(0, 1)\n",
    "        img2d = np.reshape(img1d, (responses[0].height, responses[0].width))\n",
    "\n",
    "        from PIL import Image\n",
    "\n",
    "        image = Image.fromarray(img2d)\n",
    "        im_final = np.array(image.resize((84, 84)).convert(\"L\"))\n",
    "\n",
    "        return im_final.reshape([84, 84, 1])\n",
    "\n",
    "    def _get_obs(self):\n",
    "        responses = self.drone.simGetImages([self.image_request])\n",
    "        image = self.transform_obs(responses)\n",
    "        self.drone_state = self.drone.getMultirotorState()\n",
    "\n",
    "        self.state[\"prev_position\"] = self.state[\"position\"]\n",
    "        self.state[\"position\"] = self.drone_state.kinematics_estimated.position\n",
    "        self.state[\"velocity\"] = self.drone_state.kinematics_estimated.linear_velocity\n",
    "\n",
    "        collision = self.drone.simGetCollisionInfo().has_collided\n",
    "        self.state[\"collision\"] = collision\n",
    "        return image\n",
    "        \n",
    "\n",
    "    def _do_action(self, action):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        if action == 0:  # turn left\n",
    "            self.drone.rotateByYawRateAsync(-8, 1)#.join()\n",
    "        elif action == 1:  # turn right\n",
    "            self.drone.rotateByYawRateAsync(8, 1)#.join()\n",
    "        elif action == 2:  # forward\n",
    "            future = self.drone.moveByVelocityBodyFrameAsync(5, 0, 0, 100)\n",
    "#             future = self.drone.moveByVelocityBodyFrameAsync(5, 0, 0, 1).join()\n",
    "            #for vel in np.linspace(5, 0, 3):\n",
    "            #    future = self.drone.moveByVelocityBodyFrameAsync(vel, 0, 0, 2)\n",
    "            #future.join()\n",
    "        time.sleep(1)\n",
    "\n",
    "    def _compute_reward(self):\n",
    "        thresh_dist = 7\n",
    "        beta = 1\n",
    "\n",
    "        z = -10\n",
    "        pts = [\n",
    "            np.array([-0.55265, -31.9786, -19.0225]),\n",
    "            np.array([48.59735, -63.3286, -60.07256]),\n",
    "            np.array([193.5974, -55.0786, -46.32256]),\n",
    "            np.array([369.2474, 35.32137, -62.5725]),\n",
    "            np.array([541.3474, 143.6714, -32.07256]),\n",
    "        ]\n",
    "\n",
    "        quad_pt = np.array(\n",
    "            list(\n",
    "                (\n",
    "                    self.state[\"position\"].x_val,\n",
    "                    self.state[\"position\"].y_val,\n",
    "                    self.state[\"position\"].z_val,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.state[\"collision\"]:\n",
    "            reward = -100\n",
    "        else:\n",
    "            dist = 10000000\n",
    "            for i in range(0, len(pts) - 1):\n",
    "                dist = min(\n",
    "                    dist,\n",
    "                    np.linalg.norm(np.cross((quad_pt - pts[i]), (quad_pt - pts[i + 1])))\n",
    "                    / np.linalg.norm(pts[i] - pts[i + 1]),\n",
    "                )\n",
    "\n",
    "            if dist > thresh_dist:\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward_dist = math.exp(-beta * dist) - 0.5\n",
    "                reward_speed = (\n",
    "                    np.linalg.norm(\n",
    "                        [\n",
    "                            self.state[\"velocity\"].x_val,\n",
    "                            self.state[\"velocity\"].y_val,\n",
    "                            self.state[\"velocity\"].z_val,\n",
    "                        ]\n",
    "                    )\n",
    "                    - 0.5\n",
    "                )\n",
    "                reward = reward_dist + reward_speed\n",
    "\n",
    "        done = 0\n",
    "        if reward <= -10:\n",
    "            done = 1\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def step(self, action):\n",
    "        self._do_action(action)\n",
    "        obs = self._get_obs()\n",
    "        reward, done = self._compute_reward()\n",
    "\n",
    "        return obs, reward, done, self.state\n",
    "\n",
    "    def reset(self):\n",
    "        self._setup_flight()\n",
    "        return self._get_obs(), self.state\n",
    "\n",
    "session_id = wandb.sdk.lib.runid.generate_id()\n",
    "# env = AirSimDroneEnv('137.68.192.71', 1, (84, 84))\n",
    "env = AirSimDroneEnv('127.0.0.1', 1, (84, 84))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3051ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf96e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomRotation, CenterCrop, Resize\n",
    "from torchvision.transforms.functional import rotate\n",
    "augment = Compose([\n",
    "    # RandomRotation(5),\n",
    "    CenterCrop(75),\n",
    "    Resize(resize_res)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174f3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_queue = deque(maxlen=frame_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad3cda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(img, num_level):\n",
    "    alpha_q = 0\n",
    "    beta_q = num_level - 1\n",
    "    alpha = 0\n",
    "    beta = 255\n",
    "\n",
    "    s = (beta - alpha) / (beta_q - alpha_q)\n",
    "    z = round((beta * alpha_q - alpha * beta_q) / (beta - alpha))\n",
    "\n",
    "    return (img / s).round() + z\n",
    "\n",
    "\n",
    "def quantize_with_thresholds(img, thresh_list):\n",
    "    new_img = torch.zeros_like(img)\n",
    "    n_levels = len(thresh_list) + 1\n",
    "    for i in range(1, n_levels):\n",
    "        new_img[img > thresh_list[i-1]] = i\n",
    "    return new_img\n",
    "    \n",
    "\n",
    "def process_state(state):\n",
    "    state = torch.as_tensor(state).float().view(84, 84)\n",
    "    state_queue.append(state)\n",
    "    \n",
    "    stack = list(state_queue)\n",
    "    if len(stack) < frame_stack:\n",
    "        stack = stack + [stack[-1]]*(frame_stack-len(stack))\n",
    "    stack = torch.stack(stack)\n",
    "    return augment(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a9e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee Geonju\\AppData\\Local\\Temp\\ipykernel_12860\\1859816210.py:57: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  img1d = np.array(responses[0].image_data_float, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "rollouts = []\n",
    "state = process_state(env.reset()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c0dab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'rollouts': rollouts}, 'spirl_rollouts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9fd633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee Geonju\\AppData\\Local\\Temp\\ipykernel_12860\\1859816210.py:57: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  img1d = np.array(responses[0].image_data_float, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "state_queue = deque(maxlen=frame_stack)\n",
    "state, info = env.reset()\n",
    "state = process_state(state)\n",
    "position = torch.tensor((info['position'].x_val, info['position'].y_val, info['position'].z_val,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264b11c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: skill updated tensor([2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee Geonju\\AppData\\Local\\Temp\\ipykernel_12860\\1859816210.py:57: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  img1d = np.array(responses[0].image_data_float, dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: skill updated tensor([2, 2, 1])\n",
      "step 7: skill updated tensor([0, 0, 2])\n",
      "step 10: skill updated tensor([0, 0, 0])\n",
      "step 13: skill updated tensor([0, 0, 0])\n",
      "step 16: skill updated tensor([0, 0, 0])\n",
      "step 19: skill updated tensor([0, 0, 0])\n",
      "step 22: skill updated tensor([0, 0, 0])\n",
      "step 25: skill updated tensor([0, 0, 0])\n",
      "step 28: skill updated tensor([0, 0, 0])\n",
      "step 31: skill updated tensor([2, 2, 2])\n",
      "step 34: skill updated tensor([2, 2, 2])\n",
      "step 37: skill updated tensor([2, 0, 0])\n",
      "step 40: skill updated tensor([2, 1, 1])\n",
      "step 43: skill updated tensor([1, 1, 1])\n",
      "step 46: skill updated tensor([1, 2, 1])\n",
      "step 49: skill updated tensor([2, 2, 0])\n",
      "step 52: skill updated tensor([2, 0, 0])\n",
      "step 55: skill updated tensor([0, 0, 0])\n",
      "step 58: skill updated tensor([0, 0, 0])\n",
      "step 61: skill updated tensor([1, 1, 1])\n",
      "step 64: skill updated tensor([1, 2, 1])\n",
      "step 67: skill updated tensor([2, 1, 1])\n",
      "step 70: skill updated tensor([0, 2, 0])\n",
      "step 73: skill updated tensor([0, 0, 0])\n",
      "step 76: skill updated tensor([0, 0, 0])\n",
      "step 79: skill updated tensor([2, 2, 2])\n",
      "step 82: skill updated tensor([1, 1, 1])\n",
      "step 85: skill updated tensor([1, 1, 1])\n",
      "step 88: skill updated tensor([2, 1, 1])\n",
      "step 91: skill updated tensor([2, 1, 1])\n",
      "step 94: skill updated tensor([1, 1, 1])\n",
      "step 97: skill updated tensor([1, 1, 1])\n",
      "step 100: skill updated tensor([1, 1, 1])\n",
      "step 103: skill updated tensor([1, 1, 2])\n",
      "step 106: skill updated tensor([2, 2, 1])\n",
      "step 109: skill updated tensor([0, 0, 0])\n",
      "step 112: skill updated tensor([0, 0, 0])\n",
      "step 115: skill updated tensor([0, 0, 2])\n",
      "step 118: skill updated tensor([2, 0, 2])\n",
      "step 121: skill updated tensor([2, 2, 1])\n",
      "step 124: skill updated tensor([0, 0, 0])\n",
      "step 127: skill updated tensor([2, 1, 1])\n",
      "step 130: skill updated tensor([1, 1, 1])\n",
      "step 133: skill updated tensor([1, 1, 2])\n",
      "step 136: skill updated tensor([0, 0, 0])\n",
      "step 139: skill updated tensor([2, 1, 1])\n",
      "step 142: skill updated tensor([1, 1, 2])\n",
      "step 145: skill updated tensor([2, 2, 2])\n",
      "step 148: skill updated tensor([2, 2, 2])\n",
      "step 151: skill updated tensor([2, 1, 1])\n",
      "step 154: skill updated tensor([1, 1, 1])\n",
      "step 157: skill updated tensor([0, 0, 0])\n",
      "step 160: skill updated tensor([2, 1, 1])\n",
      "step 163: skill updated tensor([0, 0, 0])\n",
      "step 166: skill updated tensor([2, 0, 0])\n",
      "step 169: skill updated tensor([0, 0, 0])\n",
      "step 172: skill updated tensor([2, 2, 2])\n",
      "step 175: skill updated tensor([0, 0, 0])\n",
      "step 178: skill updated tensor([1, 1, 1])\n",
      "step 181: skill updated tensor([1, 2, 2])\n",
      "step 184: skill updated tensor([0, 0, 0])\n",
      "step 187: skill updated tensor([2, 2, 2])\n",
      "step 190: skill updated tensor([1, 1, 1])\n",
      "step 193: skill updated tensor([0, 0, 0])\n",
      "step 196: skill updated tensor([0, 0, 0])\n",
      "step 199: skill updated tensor([1, 2, 1])\n",
      "step 202: skill updated tensor([1, 1, 1])\n",
      "step 205: skill updated tensor([2, 1, 1])\n",
      "step 208: skill updated tensor([1, 1, 1])\n",
      "step 211: skill updated tensor([1, 1, 1])\n",
      "step 214: skill updated tensor([1, 1, 1])\n",
      "step 217: skill updated tensor([1, 1, 2])\n",
      "step 220: skill updated tensor([1, 1, 1])\n",
      "step 223: skill updated tensor([1, 2, 1])\n",
      "step 226: skill updated tensor([1, 1, 1])\n",
      "step 229: skill updated tensor([1, 1, 1])\n",
      "step 232: skill updated tensor([0, 2, 2])\n",
      "step 235: skill updated tensor([2, 2, 0])\n",
      "step 238: skill updated tensor([0, 0, 0])\n",
      "step 241: skill updated tensor([0, 0, 0])\n",
      "step 244: skill updated tensor([0, 2, 2])\n",
      "step 247: skill updated tensor([2, 2, 2])\n",
      "step 250: skill updated tensor([2, 2, 2])\n",
      "step 253: skill updated tensor([2, 2, 0])\n",
      "step 256: skill updated tensor([2, 2, 2])\n",
      "step 259: skill updated tensor([1, 1, 1])\n",
      "step 262: skill updated tensor([1, 1, 1])\n",
      "step 265: skill updated tensor([1, 2, 2])\n",
      "step 268: skill updated tensor([2, 2, 1])\n",
      "step 271: skill updated tensor([1, 2, 1])\n",
      "step 274: skill updated tensor([1, 1, 2])\n",
      "step 277: skill updated tensor([1, 1, 1])\n",
      "step 280: skill updated tensor([1, 1, 1])\n",
      "step 283: skill updated tensor([0, 0, 0])\n",
      "step 286: skill updated tensor([2, 1, 1])\n",
      "step 289: skill updated tensor([1, 1, 1])\n",
      "step 292: skill updated tensor([1, 1, 1])\n",
      "step 295: skill updated tensor([2, 2, 2])\n",
      "step 298: skill updated tensor([2, 1, 1])\n",
      "step 301: skill updated tensor([0, 0, 0])\n",
      "step 304: skill updated tensor([2, 0, 1])\n",
      "step 307: skill updated tensor([1, 1, 2])\n",
      "step 310: skill updated tensor([0, 0, 0])\n",
      "step 313: skill updated tensor([1, 1, 1])\n",
      "step 316: skill updated tensor([2, 0, 0])\n",
      "step 319: skill updated tensor([0, 0, 0])\n",
      "step 322: skill updated tensor([0, 0, 0])\n",
      "step 325: skill updated tensor([2, 2, 1])\n",
      "step 328: skill updated tensor([0, 0, 0])\n",
      "step 331: skill updated tensor([0, 2, 0])\n",
      "step 334: skill updated tensor([0, 2, 2])\n",
      "step 337: skill updated tensor([0, 2, 2])\n",
      "step 340: skill updated tensor([0, 0, 0])\n",
      "step 343: skill updated tensor([0, 2, 0])\n",
      "step 346: skill updated tensor([2, 2, 2])\n",
      "step 349: skill updated tensor([0, 2, 0])\n",
      "step 352: skill updated tensor([0, 0, 0])\n",
      "step 355: skill updated tensor([2, 0, 2])\n",
      "step 358: skill updated tensor([2, 1, 1])\n",
      "step 358 collision!, 1 time(s) collided\n",
      "step 359: skill updated tensor([2, 0, 0])\n",
      "step 362: skill updated tensor([2, 2, 2])\n",
      "step 365: skill updated tensor([2, 2, 1])\n",
      "step 368: skill updated tensor([2, 1, 2])\n",
      "step 371: skill updated tensor([1, 1, 1])\n",
      "step 371 collision!, 2 time(s) collided\n",
      "step 372: skill updated tensor([2, 1, 2])\n",
      "step 375: skill updated tensor([0, 0, 0])\n",
      "step 378: skill updated tensor([2, 2, 1])\n",
      "step 381: skill updated tensor([0, 0, 0])\n",
      "step 384: skill updated tensor([0, 0, 0])\n",
      "step 387: skill updated tensor([0, 0, 0])\n",
      "step 390: skill updated tensor([0, 2, 2])\n",
      "step 393: skill updated tensor([2, 2, 2])\n",
      "step 396: skill updated tensor([2, 2, 2])\n",
      "step 399: skill updated tensor([2, 0, 0])\n",
      "step 402: skill updated tensor([0, 0, 0])\n",
      "step 405: skill updated tensor([2, 2, 2])\n",
      "step 408: skill updated tensor([0, 0, 0])\n",
      "step 411: skill updated tensor([0, 0, 0])\n",
      "step 414: skill updated tensor([0, 0, 0])\n",
      "step 417: skill updated tensor([2, 2, 2])\n",
      "step 420: skill updated tensor([2, 2, 2])\n",
      "step 423: skill updated tensor([0, 0, 0])\n",
      "step 426: skill updated tensor([0, 0, 0])\n",
      "step 429: skill updated tensor([0, 2, 2])\n",
      "step 432: skill updated tensor([2, 2, 0])\n",
      "step 435: skill updated tensor([0, 2, 2])\n",
      "step 438: skill updated tensor([2, 0, 0])\n",
      "step 441: skill updated tensor([0, 0, 0])\n",
      "step 444: skill updated tensor([0, 0, 0])\n",
      "step 447: skill updated tensor([0, 0, 2])\n",
      "step 450: skill updated tensor([0, 0, 0])\n",
      "step 453: skill updated tensor([1, 1, 1])\n",
      "step 456: skill updated tensor([1, 1, 1])\n",
      "step 459: skill updated tensor([1, 1, 2])\n",
      "step 462: skill updated tensor([2, 2, 2])\n",
      "step 465: skill updated tensor([0, 0, 2])\n",
      "step 468: skill updated tensor([2, 2, 2])\n",
      "step 471: skill updated tensor([0, 0, 0])\n",
      "step 474: skill updated tensor([0, 0, 0])\n",
      "step 477: skill updated tensor([0, 0, 0])\n",
      "step 480: skill updated tensor([0, 0, 0])\n",
      "step 483: skill updated tensor([0, 0, 2])\n",
      "step 486: skill updated tensor([0, 0, 0])\n",
      "step 489: skill updated tensor([0, 0, 0])\n",
      "step 492: skill updated tensor([0, 0, 0])\n",
      "step 495: skill updated tensor([2, 2, 2])\n",
      "step 498: skill updated tensor([2, 2, 2])\n",
      "step 501: skill updated tensor([2, 2, 1])\n",
      "step 504: skill updated tensor([1, 1, 1])\n",
      "step 507: skill updated tensor([2, 2, 0])\n",
      "step 510: skill updated tensor([2, 2, 2])\n",
      "step 513: skill updated tensor([0, 0, 0])\n",
      "step 516: skill updated tensor([2, 1, 1])\n",
      "step 519: skill updated tensor([0, 0, 0])\n",
      "step 522: skill updated tensor([0, 0, 0])\n",
      "step 525: skill updated tensor([1, 1, 1])\n",
      "step 528: skill updated tensor([2, 2, 2])\n",
      "step 531: skill updated tensor([2, 1, 1])\n",
      "step 534: skill updated tensor([1, 1, 1])\n",
      "step 537: skill updated tensor([0, 0, 0])\n",
      "step 540: skill updated tensor([0, 0, 0])\n",
      "step 543: skill updated tensor([0, 0, 0])\n",
      "step 546: skill updated tensor([0, 0, 0])\n",
      "step 549: skill updated tensor([0, 0, 0])\n",
      "step 552: skill updated tensor([0, 0, 0])\n",
      "step 555: skill updated tensor([0, 0, 0])\n",
      "step 558: skill updated tensor([0, 0, 0])\n",
      "step 561: skill updated tensor([2, 0, 0])\n",
      "step 564: skill updated tensor([0, 2, 0])\n",
      "step 567: skill updated tensor([0, 0, 0])\n",
      "step 570: skill updated tensor([2, 2, 2])\n",
      "step 573: skill updated tensor([0, 0, 0])\n",
      "step 576: skill updated tensor([0, 0, 0])\n",
      "step 579: skill updated tensor([0, 0, 0])\n",
      "step 582: skill updated tensor([2, 1, 1])\n",
      "step 585: skill updated tensor([1, 2, 1])\n",
      "step 588: skill updated tensor([1, 1, 1])\n",
      "step 591: skill updated tensor([1, 1, 2])\n",
      "step 594: skill updated tensor([0, 0, 0])\n",
      "step 597: skill updated tensor([0, 0, 0])\n",
      "step 600: skill updated tensor([2, 2, 2])\n",
      "step 603: skill updated tensor([2, 1, 1])\n",
      "step 606: skill updated tensor([1, 1, 1])\n",
      "step 609: skill updated tensor([2, 0, 0])\n",
      "step 612: skill updated tensor([0, 2, 2])\n",
      "step 615: skill updated tensor([2, 2, 2])\n",
      "step 618: skill updated tensor([2, 2, 2])\n",
      "step 621: skill updated tensor([0, 0, 0])\n",
      "step 624: skill updated tensor([1, 1, 1])\n",
      "step 627: skill updated tensor([1, 2, 1])\n",
      "step 630: skill updated tensor([0, 0, 0])\n",
      "step 633: skill updated tensor([2, 2, 2])\n",
      "step 636: skill updated tensor([1, 1, 1])\n",
      "step 639: skill updated tensor([1, 2, 1])\n",
      "step 642: skill updated tensor([1, 2, 1])\n",
      "step 645: skill updated tensor([0, 0, 0])\n",
      "step 648: skill updated tensor([2, 1, 1])\n",
      "step 651: skill updated tensor([1, 1, 1])\n",
      "step 654: skill updated tensor([1, 2, 2])\n",
      "step 657: skill updated tensor([2, 2, 2])\n",
      "step 660: skill updated tensor([1, 2, 1])\n",
      "step 663: skill updated tensor([2, 2, 0])\n",
      "step 666: skill updated tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "positions = []\n",
    "collision_count = 0\n",
    "idx = 0\n",
    "for i in range(1, 3001):\n",
    "    if idx % skill_length == 0:\n",
    "        if pos:\n",
    "            z = prior_policy.dist(state, position).sample()[None, :].expand(1, skill_length, -1)\n",
    "        else:\n",
    "            z = prior_policy.dist(state).sample()[None, None, :].expand(1, skill_length, -1)\n",
    "#         actions = low_policy.dist(state, z).logits.argmax(-1).squeeze(0)\n",
    "        actions = low_policy.dist(z).logits.argmax(-1).squeeze(0)\n",
    "        print(f'step {i}: skill updated', actions)\n",
    "        # a = low_policy.dist(state, z).logits.argmax()\n",
    "        # a = low_policy.dist(z).logits.argmax()\n",
    "    a = actions[idx%skill_length]\n",
    "    idx = (idx + 1) % skill_length\n",
    "    \n",
    "    state, _, _, info = env.step(a.detach().cpu().numpy())\n",
    "#     plt.imshow(state)\n",
    "#     plt.show()\n",
    "    position = torch.tensor((info['position'].x_val, info['position'].y_val, info['position'].z_val,))\n",
    "    state = process_state(state)\n",
    "    if is_quantized:\n",
    "        if num_level != 0:\n",
    "            if len(thresholds) != 0:\n",
    "                raise\n",
    "            state = quantize(state, num_level) / (num_level - 1)\n",
    "        elif len(thresholds) != 0:\n",
    "            state = quantize_with_thresholds(state, thresholds) / len(thresholds)\n",
    "        else:\n",
    "            raise\n",
    "    else:\n",
    "        state = state / 255\n",
    "    \n",
    "    if info['collision'] is True:\n",
    "        collision_count += 1\n",
    "        print(f'step {i} collision!, {collision_count} time(s) collided')\n",
    "        state_queue = deque(maxlen=frame_stack)\n",
    "        state, info = env.reset()\n",
    "        state = process_state(state)\n",
    "        position = torch.tensor((info['position'].x_val, info['position'].y_val, info['position'].z_val,))\n",
    "        if is_quantized:\n",
    "            state = quantize(state, num_level)\n",
    "        rollouts.append(positions)\n",
    "        positions = []\n",
    "        \n",
    "##         z = prior_policy.dist(state).sample()[None, None, :].expand(1, skill_length, -1)\n",
    "\n",
    "#         z = prior_policy.dist(state).sample()[None, :].expand(1, skill_length, -1)\n",
    "#         actions = low_policy.dist(z).logits.argmax(-1).squeeze(0)\n",
    "\n",
    "##         actions = low_policy.dist(state, z).logits.argmax(-1).squeeze(0)\n",
    "        idx = 0\n",
    "    \n",
    "    positions.append(info['position'])\n",
    "print('collided ', collision_count, ' times')\n",
    "# state, _, _, info = env.step(0)\n",
    "# state = process_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7550dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_policy.conv_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c099b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollouts = torch.load('./spirl_rollouts.pt')['rollouts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.shape, state.min(), state.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_policy.dist(state) # [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_policy.dist(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19399d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = low_policy.dist(z).logits.argmax(-1).squeeze(0)\n",
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l, r, d, u = 63, 225, -369, -81\n",
    "l, r, d, u = 20, 225, -369, -81\n",
    "plt.figure(figsize=((r - l)//32, abs(d - u)//32))\n",
    "plt.imshow(\n",
    "    (1-0.3*(1-plt.imread('map.png'))),\n",
    "    extent=(l, r, u, d)\n",
    ")\n",
    "plt.xlim(l, r);plt.ylim(u, d)\n",
    "\n",
    "for positions in rollouts[:]:\n",
    "    if len(positions) < 15:\n",
    "        continue\n",
    "    plt.plot(*np.array([\n",
    "        [position.x_val, position.y_val]\n",
    "        for position in positions\n",
    "    ]).T, c='royalblue', linewidth=.6, alpha=0.6)\n",
    "    plt.scatter(positions[0].x_val, positions[0].y_val, marker='^', c='black', s=100)\n",
    "    plt.scatter(positions[-1].x_val, positions[-1].y_val, marker='x', c='red', s=100, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542bcd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i, positions in enumerate(rollouts):\n",
    "    if len(positions) > max_len:\n",
    "        max_len = len(positions)\n",
    "        print(f'max_len:{max_len}, idx: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99425dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
