{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f0d3d6",
   "metadata": {},
   "source": [
    "early stopping\n",
    "\n",
    "save ckpt\n",
    "\n",
    "tune reg_loss weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea77b18-9718-4ed0-9cfb-94fceb2eab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d1/geonju/anaconda3/envs/rl/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as torch_dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from tqdm.notebook import trange\n",
    "import cv2\n",
    "import wandb\n",
    "\n",
    "# from simpl.collector import ConcurrentCollector, TimeLimitCollector, GPUWorker, Buffer\n",
    "# from simpl.nn import itemize\n",
    "# from simpl.math import discount\n",
    "# from simpl.rl.policy import ContextTruncatedNormalMLPPolicy\n",
    "# from simpl.rl.qf import MLPQF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9fd78",
   "metadata": {},
   "source": [
    "'stereo','rgbs','actions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a390f3-cedb-42d7-9119-58580b787a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_dir = '../collect/outdoor_v1'\n",
    "rollouts = []\n",
    "for folder in os.listdir(rollout_dir):\n",
    "    rollouts += [\n",
    "        torch.load(f'{rollout_dir}/{folder}/{filename}') for filename in os.listdir(f'{rollout_dir}/{folder}') if filename[-3:] == '.pt'\n",
    "        ]\n",
    "\n",
    "# lengths = [\n",
    "#     len(rollout['actions'])\n",
    "#     for rollout in rollouts\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25906ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_int(data):\n",
    "    new_data = []\n",
    "    for action in data:\n",
    "        if action == 'w':\n",
    "            new_data.append(0)\n",
    "        elif action == 's':\n",
    "            new_data.append(1)\n",
    "        elif action == 'a':\n",
    "            new_data.append(2)\n",
    "        elif action == 'd':\n",
    "            new_data.append(3)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6e03cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51, 480, 640, 3),\n",
       " (51, 768, 1024, 3),\n",
       " array([0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 3, 3, 3, 3,\n",
       "        3, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 3, 3, 3,\n",
       "        0, 2, 2, 0, 0, 3]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0]['stereo'].shape, rollouts[0]['rgbs'].shape, action_to_int(rollouts[0]['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebb624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 480, 640, 3) (51, 768, 1024, 3) (50,)\n",
      "(51, 480, 640, 3) (51, 768, 1024, 3) (51,)\n",
      "(51, 480, 640, 3) (51, 768, 1024, 3) (50,)\n",
      "(51, 480, 640, 3) (51, 768, 1024, 3) (51,)\n",
      "(51, 480, 640, 3) (51, 768, 1024, 3) (50,)\n",
      "(51, 480, 640, 3) (51, 768, 1024, 3) (51,)\n",
      "(51, 480, 640, 3) (51, 768, 1024, 3) (50,)\n"
     ]
    }
   ],
   "source": [
    "for rollout in rollouts:\n",
    "    print(rollout['stereo'].shape, rollout['rgbs'].shape, rollout['actions'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60edda4-2131-4c44-9e5c-0f8cdda49295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([350, 480, 640]),\n",
       " torch.Size([350, 768, 1024]),\n",
       " torch.Size([350]),\n",
       " 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import Grayscale\n",
    "\n",
    "data_stereos = Grayscale(1)(torch.as_tensor(np.concatenate([\n",
    "    rollout['stereo'][:-1]\n",
    "    for rollout in rollouts\n",
    "]), dtype=torch.float32).permute(0, 3, 1, 2)).squeeze()\n",
    "\n",
    "data_rgbs = Grayscale(1)(torch.as_tensor(np.concatenate([\n",
    "    rollout['rgbs'][:-1]\n",
    "    for rollout in rollouts\n",
    "]), dtype=torch.float32).permute(0, 3, 1, 2)).squeeze()\n",
    "\n",
    "data_actions = torch.as_tensor(np.concatenate([ # map actions (char) to int first\n",
    "    action_to_int(rollout['actions'])[:50] # we need to check data collection code\n",
    "    for rollout in rollouts\n",
    "]), dtype=torch.long)\n",
    "\n",
    "lengths = [\n",
    "    len(rollout['actions'][:50])\n",
    "    for rollout in rollouts\n",
    "]\n",
    "\n",
    "data_stereos.shape, data_rgbs.shape, data_actions.shape, len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f387ca32-cad5-43d6-b4d6-c66196f71180",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_length = 5\n",
    "frame_stack = 5\n",
    "\n",
    "data_available_indices = []\n",
    "rollout_start_idx = 0\n",
    "for length in lengths:\n",
    "    available_indices = torch.arange(frame_stack-1, length-skill_length+1)\n",
    "    data_available_indices.append(rollout_start_idx + available_indices)\n",
    "    rollout_start_idx += length\n",
    "data_available_indices = torch.cat(data_available_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c0e1d3-6693-4ebd-bccb-1421a450c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_boxes(img, max_k, size=16):\n",
    "    h,w = size,size\n",
    "    img_size = img.shape[-2]\n",
    "    boxes = []\n",
    "    n_k = np.random.randint(max_k)\n",
    "    for k in range(n_k):\n",
    "        y,x = np.random.randint(0,img_size-w,(2,))\n",
    "        \n",
    "        img[:, y:y+h,x:x+w] = 0\n",
    "        boxes.append((x,y,h,w))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d0815c2-ff39-46d0-8955-899b9df64636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 37\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, BatchSampler, RandomSampler, DataLoader\n",
    "from torchvision.transforms import Compose, RandomRotation, CenterCrop, Resize\n",
    "\n",
    "class TrajDataset(Dataset):\n",
    "    augment = Compose([\n",
    "        RandomRotation(5),\n",
    "        CenterCrop(75),\n",
    "        Resize(84)\n",
    "    ])\n",
    "\n",
    "    val_augment = Compose([\n",
    "        CenterCrop(75),\n",
    "        Resize(84)\n",
    "    ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(data_available_indices)\n",
    "    \n",
    "    def __init__(self, indices, validate=False):\n",
    "        self.indices = indices\n",
    "        self.validate = validate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx_of_available_indices):\n",
    "        if type(idx_of_available_indices) != int:\n",
    "            raise\n",
    "        idx = self.indices[idx_of_available_indices]\n",
    "        seq_state = data_stereos[idx-frame_stack+1:idx+skill_length] # stereo, rgbs\n",
    "        \n",
    "        stack_seq_state = []\n",
    "        for stack_i in range(frame_stack):\n",
    "            stack_seq_state.append(seq_state[stack_i:stack_i+skill_length])\n",
    "        stack_seq_state = torch.stack(stack_seq_state).swapaxes(0, 1)\n",
    "        \n",
    "        seq_action = data_actions[idx:idx+skill_length]\n",
    "        \n",
    "        if not self.validate: # augmentation\n",
    "            stack_seq_state = self.augment(add_random_boxes(stack_seq_state, 3))\n",
    "        else:\n",
    "            stack_seq_state = self.augment(stack_seq_state)\n",
    "        \n",
    "        return stack_seq_state / 255, seq_action\n",
    "\n",
    "training_indices, val_indices = data_available_indices[:250], data_available_indices[250:]\n",
    "\n",
    "dataset, val_dataset = TrajDataset(training_indices), TrajDataset(val_indices, True)\n",
    "print(len(dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e04dc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 84, 84])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7bbdf",
   "metadata": {},
   "source": [
    "Should we get a depth estimation model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcbeb8d-cd80-449a-ade7-00ea5d21ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from simpl.nn import MLP\n",
    "\n",
    "from simpl.nn import ToDeviceMixin\n",
    "import torch.distributions as torch_dist\n",
    "\n",
    "from simpl.math import inverse_softplus, inverse_sigmoid\n",
    "\n",
    "\n",
    "class SkillEncoder(ToDeviceMixin, nn.Module):\n",
    "    def __init__(self, action_dim, z_dim, hidden_dim, n_lstm, n_mlp_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            action_dim,\n",
    "            hidden_dim, n_lstm, batch_first=True\n",
    "        )\n",
    "        self.mlp = MLP([hidden_dim]*n_mlp_hidden + [2*z_dim], 'relu')\n",
    "        \n",
    "        self.register_buffer('prior_loc', torch.zeros(z_dim))\n",
    "        self.register_buffer('prior_scale', torch.ones(z_dim))\n",
    "        # self.register_buffer('h0', torch.zeros(n_lstm, hidden_dim))\n",
    "        # self.register_buffer('c0', torch.zeros(n_lstm, hidden_dim))\n",
    "\n",
    "    @property\n",
    "    def prior_dist(self):\n",
    "        return torch_dist.Independent(torch_dist.Normal(self.prior_loc, self.prior_scale), 1)\n",
    "    \n",
    "        \n",
    "    def dist(self, batch_seq_action):\n",
    "        # batch_h0 = self.h0[:, None, :].expand(-1, len(batch_seq_state), -1)\n",
    "        # batch_c0 = self.c0[:, None, :].expand(-1, len(batch_seq_state), -1)\n",
    "        \n",
    "        batch_seq_onehot_action = F.one_hot(batch_seq_action, num_classes=self.action_dim).float()\n",
    "        batch_seq_out, _ = self.lstm(batch_seq_onehot_action)\n",
    "        batch_last_out = batch_seq_out[:, -1, :]\n",
    "        batch_loc, batch_pre_scale = self.mlp(batch_last_out).chunk(2, dim=-1)\n",
    "        batch_scale = F.softplus(batch_pre_scale)\n",
    "        \n",
    "        return torch_dist.Independent(\n",
    "            torch_dist.Normal(batch_loc, batch_scale)\n",
    "        , 1)\n",
    "\n",
    "class PriorPolicy(ToDeviceMixin, nn.Module):\n",
    "    def __init__(self, state_shape, z_dim, hidden_dim, n_hidden,\n",
    "                 min_scale=0.001, max_scale=None, init_scale=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert state_shape == (5, 84, 84)\n",
    "        assert hidden_dim == 128\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(5, 32, kernel_size=4, stride=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.mlp = MLP([128]*n_hidden + [2*z_dim], 'relu')\n",
    "        \n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        \n",
    "        if max_scale is None:\n",
    "            self.pre_init_scale = inverse_softplus(init_scale)\n",
    "        else:\n",
    "            self.pre_init_scale = inverse_sigmoid(init_scale / max_scale)\n",
    "        \n",
    "    def dist(self, batch_state):\n",
    "        input_dim = batch_state.dim()\n",
    "        if input_dim > 4:\n",
    "            batch_shape = batch_state.shape[:-3] \n",
    "            data_shape = batch_state.shape[-3:]\n",
    "            batch_state = batch_state.view(-1, *data_shape)\n",
    "        \n",
    "        batch_h = self.conv_net(batch_state)[..., 0, 0]\n",
    "        batch_loc, batch_pre_scale = self.mlp(batch_h).chunk(2, dim=-1)\n",
    "\n",
    "        if self.max_scale is None:\n",
    "            batch_scale = self.min_scale + F.softplus(self.pre_init_scale + batch_pre_scale)\n",
    "        else:\n",
    "            batch_scale = self.min_scale + self.max_scale*torch.sigmoid(self.pre_init_scale + batch_pre_scale)\n",
    "        \n",
    "        if input_dim > 4:\n",
    "            batch_loc = batch_loc.view(*batch_shape, self.z_dim)\n",
    "            batch_scale = batch_scale.view(*batch_shape, self.z_dim)\n",
    "        \n",
    "        return torch_dist.Independent(\n",
    "            torch_dist.Normal(batch_loc, batch_scale)\n",
    "        , 1)\n",
    "        \n",
    "class LowPolicy(ToDeviceMixin, nn.Module):\n",
    "    def __init__(self, state_shape, action_dim, z_dim, hidden_dim, n_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert state_shape == (5, 84, 84)\n",
    "        assert hidden_dim == 128\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(5, 32, kernel_size=4, stride=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.mlp = MLP([128 + z_dim] + [hidden_dim]*(n_hidden-1) + [action_dim], 'relu')\n",
    "        \n",
    "    def dist(self, batch_state, batch_z):\n",
    "        input_dim = batch_state.dim()\n",
    "        if input_dim > 4:\n",
    "            batch_shape = batch_state.shape[:-3]\n",
    "            \n",
    "            data_shape = batch_state.shape[len(batch_shape):]\n",
    "            batch_state = batch_state.view(-1, *data_shape)\n",
    "\n",
    "            data_shape = batch_z.shape[len(batch_shape):]\n",
    "            batch_z = batch_z.reshape(-1, *data_shape)\n",
    "        \n",
    "        batch_h = self.conv_net(batch_state)[..., 0, 0]\n",
    "        batch_h_z = torch.cat([batch_h, batch_z], dim=-1)\n",
    "        batch_logits = self.mlp(batch_h_z)\n",
    "        \n",
    "        if input_dim > 4:\n",
    "            batch_logits = batch_logits.view(*batch_shape, self.action_dim)\n",
    "        \n",
    "        return torch_dist.Categorical(logits=batch_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69def1bb-6c8e-4299-8f0b-f607bd4e4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    encoder=dict(hidden_dim=256, n_lstm=2, n_mlp_hidden=2),\n",
    "    prior_policy=dict(hidden_dim=128, n_hidden=2, init_scale=1, max_scale=2),\n",
    "    low_policy=dict(hidden_dim=128, n_hidden=2),\n",
    "    reuse_rate=128,\n",
    "    batch_size=128,\n",
    "    z_dim=5,\n",
    "    reg_scale=5e-4\n",
    ")\n",
    "gpu = 1\n",
    "state_shape = (5, 84, 84)\n",
    "action_dim = 3\n",
    "z_dim = config['z_dim']\n",
    "\n",
    "filename = 'pretrain.spirl_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fdd83-941c-4079-ae6c-59863fffe03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SkillEncoder(action_dim, z_dim, **config['encoder']).to(gpu)\n",
    "prior_policy = PriorPolicy(state_shape, z_dim, **config['prior_policy']).to(gpu)\n",
    "low_policy = LowPolicy(state_shape, action_dim, z_dim, **config['low_policy']).to(gpu)\n",
    "\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters(), lr=3e-4)\n",
    "prior_policy_optim = torch.optim.Adam(prior_policy.parameters(), lr=3e-4)\n",
    "low_policy_optim = torch.optim.Adam(low_policy.parameters(), lr=3e-4)\n",
    "\n",
    "skill_prior_dist = torch_dist.Independent(torch_dist.Normal(\n",
    "    torch.zeros(z_dim).to(gpu),\n",
    "    torch.ones(z_dim).to(gpu),\n",
    "), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6708190-9c64-4b94-991c-b40627a89b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=config['batch_size'],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config['batch_size'],\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(loader))[0][1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62952f3f-d926-4792-85f1-29712946f955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project='drone', entity='mlai-rl', config=config)\n",
    "\n",
    "wandb.run.name = f'{filename}_{wandb.run.id}'\n",
    "wandb.run.save()\n",
    "save_filename = wandb.run.name.split('/')[-1] + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder, prior_policy, low_policy, log):\n",
    "\n",
    "    num_success = 0\n",
    "    val_recon_loss = 0\n",
    "    val_prior_loss = 0\n",
    "    val_reg_loss = 0\n",
    "    val_loss = 0\n",
    "    val_action_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step_i, (batch_seq_state, batch_seq_action) in enumerate(val_loader):\n",
    "            batch_seq_state = batch_seq_state.to(gpu)\n",
    "            batch_seq_action = batch_seq_action.to(gpu)\n",
    "            \n",
    "            batch_skill_dist = encoder.dist(batch_seq_action)\n",
    "            batch_skill_prior_dist = prior_policy.dist(batch_seq_state[:, 0, :])\n",
    "            \n",
    "            batch_skill = batch_skill_dist.rsample()\n",
    "            batch_seq_skill = batch_skill[:, None, :].expand(-1, skill_length, -1)\n",
    "\n",
    "            batch_seq_policy_dist = low_policy.dist(batch_seq_state, batch_seq_skill)\n",
    "\n",
    "            recon_loss = -batch_seq_policy_dist.log_prob(batch_seq_action).mean((0, 1))\n",
    "            \n",
    "            reg_loss = torch_dist.kl_divergence(\n",
    "                batch_skill_dist,\n",
    "                skill_prior_dist\n",
    "            ).mean(0)\n",
    "            \n",
    "            prior_loss = - batch_skill_prior_dist.log_prob(batch_skill_dist.sample()).mean(0)\n",
    "            # batch_skill_dist.base_dist.loc = batch_skill_dist.base_dist.loc.detach()\n",
    "            # batch_skill_dist.base_dist.scale = batch_skill_dist.base_dist.scale.detach()\n",
    "            # prior_loss = torch_dist.kl_divergence(\n",
    "            #     batch_skill_dist,\n",
    "            #     batch_skill_prior_dist\n",
    "            # ).mean(0)\n",
    "            \n",
    "            loss = recon_loss + prior_loss + config['reg_scale']*reg_loss\n",
    "\n",
    "            val_recon_loss += recon_loss\n",
    "            val_reg_loss += reg_loss\n",
    "            val_prior_loss += prior_loss\n",
    "            val_loss += loss\n",
    "            \n",
    "            action_accuracy = (batch_seq_policy_dist.logits.argmax(-1) == batch_seq_action).float().mean()\n",
    "            val_action_accuracy += action_accuracy\n",
    "\n",
    "        log.update({\n",
    "            'val_action_acc': val_action_accuracy / (step_i+1),\n",
    "            'val_recon_loss': val_recon_loss / (step_i+1),\n",
    "            'val_reg_loss': val_reg_loss / (step_i+1),\n",
    "            'val_prior_loss': val_prior_loss / (step_i+1),\n",
    "            'val_loss': val_loss / (step_i+1),\n",
    "            'val_skill_prior_ent': -batch_skill_prior_dist.log_prob(batch_skill_dist.mean).mean(0),\n",
    "            })\n",
    "\n",
    "        return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c29d6-6682-40c4-a54d-001465c14493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for epoch_i in range(1, 5001):\n",
    "    log = {'epoch_i': epoch_i}\n",
    "\n",
    "    for step_i, (batch_seq_state, batch_seq_action) in enumerate(loader):\n",
    "        batch_seq_state = batch_seq_state.to(gpu)\n",
    "        batch_seq_action = batch_seq_action.to(gpu)\n",
    "        \n",
    "        batch_skill_dist = encoder.dist(batch_seq_action)\n",
    "        batch_skill_prior_dist = prior_policy.dist(batch_seq_state[:, 0, :])\n",
    "        \n",
    "        batch_skill = batch_skill_dist.rsample()\n",
    "        batch_seq_skill = batch_skill[:, None, :].expand(-1, skill_length, -1)\n",
    "\n",
    "        batch_seq_policy_dist = low_policy.dist(batch_seq_state, batch_seq_skill)\n",
    "\n",
    "        recon_loss = -batch_seq_policy_dist.log_prob(batch_seq_action).mean((0, 1))\n",
    "        \n",
    "        reg_loss = torch_dist.kl_divergence(\n",
    "            batch_skill_dist,\n",
    "            skill_prior_dist\n",
    "        ).mean(0)\n",
    "        \n",
    "        \n",
    "        if epoch_i < 20:\n",
    "            prior_loss = 0\n",
    "        else:\n",
    "            prior_loss = - batch_skill_prior_dist.log_prob(batch_skill_dist.sample()).mean(0)\n",
    "            # batch_skill_dist.base_dist.loc = batch_skill_dist.base_dist.loc.detach()\n",
    "            # batch_skill_dist.base_dist.scale = batch_skill_dist.base_dist.scale.detach()\n",
    "            # prior_loss = torch_dist.kl_divergence(\n",
    "            #     batch_skill_dist,\n",
    "            #     batch_skill_prior_dist\n",
    "            # ).mean(0)\n",
    "        loss = recon_loss + prior_loss + config['reg_scale']*reg_loss\n",
    "        \n",
    "        encoder_optim.zero_grad()\n",
    "        low_policy_optim.zero_grad()\n",
    "        prior_policy_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        encoder_optim.step()\n",
    "        low_policy_optim.step()\n",
    "        prior_policy_optim.step()\n",
    "\n",
    "    # validation\n",
    "    log = validation(encoder, prior_policy, low_policy, log)    \n",
    "    \n",
    "    action_accuracy = (batch_seq_policy_dist.logits.argmax(-1) == batch_seq_action).float().mean()\n",
    "    \n",
    "    log.update({\n",
    "        'loss': loss,\n",
    "        'recon_loss': recon_loss,\n",
    "        'prior_loss': prior_loss,\n",
    "        'reg_loss': reg_loss,\n",
    "        'mean_encoder_scale': batch_skill_dist.base_dist.scale.mean(),\n",
    "        'mean_prior_policy_scale': batch_skill_prior_dist.base_dist.scale.mean(),\n",
    "        'action_accuracy': action_accuracy,\n",
    "        'skill_prior_ent': -batch_skill_prior_dist.log_prob(batch_skill_dist.mean).mean(0)\n",
    "    })\n",
    "\n",
    "    wandb.log(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a5d24-dedd-4d57-a6c9-4efafc0d7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'prior_policy': prior_policy,\n",
    "    'low_policy': low_policy,\n",
    "    'encoder': encoder,\n",
    "}, f'{wandb.run.name}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d80faaa9edf940083eff052636545645c2b15b2d8dfa82e041f45649216c1fd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
